# Name: shared_services

services:
  # ------------------------------------------------------------------------------------
  # Setup Object Storage
  # ------------------------------------------------------------------------------------
  # Minio is an S3 compatible object store that can be used by any application in any
  # stack. This setup creates a Minio cluster spread across 4 nodes, each with a single
  # drive, and this placement is enforced via the node ID. Data is erasure coded such
  # that 2 drives can go down and all data can still be recovered.
  #
  # Each of the 4 minio instances are identical except for their hostname and placement
  # constraints.
  # ------------------------------------------------------------------------------------
  minio1:
    # This is required. Swarm has two deployment modes: global or replicated. Global will
    # be deployed on _every_ node, using "replicas: 1" is the only way to state that this
    # service should be a singleton, however the hostname for a service in docker swarm
    # will be "container_name.task_slot" = "minio1.1".
    hostname: minio1
    image: minio/minio:latest
    volumes:
      - minio1_data:/data
    networks:
      - shared_services
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: any
        max_attempts: 5
      placement:
        constraints:
          # Use the node id from the swarm cluster via `docker node ls`
          - node.id == omgb8iqu6i826sf4xiqc5xi1f
    # This part is extremely important, specifying the path to the servers using expansion
    # syntax is used for other minio servers to discover eachother for distributed mode.
    # We are stating that there are 4 minio servers which can be reached at:
    # - ["http://minio1", "http://minio2", "http://minio3", "http://minio4"]
    # We are also stating that there is only 1 drive for each of these servers:
    # - "http://minio1/data" where "/data" indicates the drive. If we had more drives for
    #   the servers we could use expansion syntax again: "http://minio{1...4}/data{1...2}"
    #
    # See the docs for more info:
    # https://min.io/docs/minio/linux/operations/install-deploy-manage/expand-minio-deployment.html#sequential-hostnames
    command: server http://minio{1...4}/data
    environment:
      - MINIO_ROOT_USER_FILE=/run/secrets/minio_root_user
      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/minio_root_password
      - MINIO_STORAGE_CLASS_STANDARD=EC:2 # Can withstand up to 2 drives failing.
      - MINIO_BROWSER=off # disable embedded web portal
    secrets:
      - minio_root_user
      - minio_root_password
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
  minio2:
    hostname: minio2
    image: minio/minio:latest
    volumes:
      - minio2_data:/data
    networks:
      - shared_services
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: any
        max_attempts: 5
      placement:
        constraints:
          - node.id == yml174okin9b6czgu0ignapjd
    command: server http://minio{1...4}/data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER_FILE=/run/secrets/minio_root_user
      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/minio_root_password
      - MINIO_STORAGE_CLASS_STANDARD=EC:2
      - MINIO_BROWSER=off
    secrets:
      - minio_root_user
      - minio_root_password
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
  minio3:
    hostname: minio3
    image: minio/minio:latest
    volumes:
      - minio3_data:/data
    networks:
      - shared_services
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: any
        max_attempts: 5
      placement:
        constraints:
          - node.id == epunrut5bznn7agd65gyyfjy1
    command: server http://minio{1...4}/data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER_FILE=/run/secrets/minio_root_user
      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/minio_root_password
      - MINIO_STORAGE_CLASS_STANDARD=EC:2
      - MINIO_BROWSER=off
    secrets:
      - minio_root_user
      - minio_root_password
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
  minio4:
    hostname: minio4
    image: minio/minio:latest
    volumes:
      - minio4_data:/data
    networks:
      - shared_services
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: any
        max_attempts: 5
      placement:
        constraints:
          - node.id == ox8r7ehw8p6dkw9w710sq35pd
    command: server http://minio{1...4}/data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER_FILE=/run/secrets/minio_root_user
      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/minio_root_password
      - MINIO_STORAGE_CLASS_STANDARD=EC:2
      - MINIO_BROWSER=off
    secrets:
      - minio_root_user
      - minio_root_password
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # ------------------------------------------------------------------------------------
  # Setup Observability Tools
  # ------------------------------------------------------------------------------------
  # The Grafana Stack is a FOSS observability stack that can be self hosted. There are
  # really 3 main pieces that we need to setup:
  # - Loki  - our logs database and query engine
  # - Mimir - our metrics database and query engine
  # - Alloy - our agent to collect both logs and metrics, sending to Loki & Mimir
  #
  # Grafana is such a gem that each of their services typically support up to 3 different
  # deployment modes: monolithic, simple-scalable, microservices. The solution chosen
  # here is the simple-scalable deployment, monolithic is not recommended for production
  # as scaling is wasteful (don't need whole app duplicated to increase read/write
  # throughput for example).
  #
  # Loki will be deployed using simple-scalable-deployment where we split out the read,
  # write, and backend. Each of these pieces can be scaled independently by just changing
  # the number of replicas in the compose file.
  #
  # Mimir will be deployed using read-write deployment (SSD) where we split out the read,
  # write, and backend. Each of these pieces can be scaled independently by just changing
  # the number of replicas in the compose file.
  #
  # Alloy will be deployed in Docker Swarm "global" mode. This means that there will be
  # one alloy instance running on each node. This single instance will collect all logs
  # and metrics for the node and send the data to the appropriate sink (loki or mimir).
  # ------------------------------------------------------------------------------------

  # ------------------------------------------------------------------------------------
  # Setup Reverse Proxy
  # ------------------------------------------------------------------------------------
  # This caddyserver is used as a reverse proxy & load balancer for:
  # - Minio   - all traffic should go through caddyserver for minio
  # - Apps    - publically exposed web apps are served here, these webapps are created
  #             in different stacks deployed to this swarm
  # ------------------------------------------------------------------------------------
  caddy:
    image: caddy:latest
    volumes:
      - caddy_data:/data
      - caddy_config:/config
    configs:
      - source: caddyfile
        target: /etc/caddy/Caddyfile
    ports:
      # allow for web traffic
      - 80:80
      - 443:443
      - 443:443/udp
    networks:
      - shared_services
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: any
        max_attempts: 5
      placement:
        constraints:
          # Use the node id from the swarm cluster via `docker node ls`
          - node.id == omgb8iqu6i826sf4xiqc5xi1f
    healthcheck:
      test:
        ["CMD", "wget", "--spider", "-q", "http://localhost:2019/healthcheck"]
      interval: 30s
      timeout: 20s
      retries: 3

volumes:
  minio1_data:
    driver: local
  minio2_data:
    driver: local
  minio3_data:
    driver: local
  minio4_data:
    driver: local
  caddy_data:
    driver: local
  caddy_config:
    driver: local

configs:
  caddyfile:
    external: true
    name: "shared_services.caddyfile.${CADDYFILE_VER}"

networks:
  shared_services:
    driver: overlay
    driver_opts:
      encrypted: "true"

secrets:
  minio_root_user:
    external: true
    name: "shared_services.minio_root_user.${MINIO_ROOT_VER}"
  minio_root_password:
    external: true
    name: "shared_services.minio_root_password.${MINIO_ROOT_VER}"
