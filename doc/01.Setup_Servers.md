# Infrastructure

My home network is running using a [Turing PI 2.5.1](https://turingpi.com/product/turing-pi-2-5/) cluster board with 4 [Turing RK1 32GB](https://turingpi.com/product/turing-rk1/?attribute_ram=32+GB) cluster boards. These boards are ARM chips which are great for low power usage, and have an impressive amount of compute with 8 Cores - 32GB RAM - 6 TOPS NPU each. To put into perspective how much compute this is, if I were to purchase this on [fly.io](https://fly.io/docs/about/pricing/#started-fly-machines) it would cost:

```math
4 \times (\text{performance-8x 32Gb @ \$328.04/month}) = \$1,312.16/\text{month} = \$15,745.92/\text{year}
```

or on AWS it would cost:

```math
4 \times (\text{m6g.2xlarge 32Gb @ \$225.08/month}) = \$900.32/\text{month} = \$10,803.84/\text{year}
```

## Network

The network is a 1Gbps (125MB/s) fiber connection to the house with an Eero 6 router. The Turing Pi 2.5 has 2 1Gbps ethernet ports, and only one of them is plugged into the Eero, so max network traffic is:

```math
\text{1Gbps or 125MB/s}
```

The max bandwidth from my ISP is also 1Gbps so plugging in the extra ethernet port would have no effect. I have also requested a static IP from my ISP and am using that plus Cloudflare DNS to route traffic through Cloudflare to my home network.

## Server

Each of the Turing RK1 compute modules is running Ubuntu 24.04. The following commands were used to set up each container from fresh Ubuntu install.

### SSH Keys Only

To secure access to each of the machines, an SSH key was added for login and basic auth was turned off.

1. Add my public SSH key to the `~/.ssh/authorized_keys` file on a new line.
2. Update the `/etc/ssh/sshd_config` making the following changes:

```bash
PasswordAuthentication no
UsePAM no
```

3. Restart the `ssh` server

```bash
sudo systemctl restart ssh
```

### Kubernetes

All of the Kubernetes prerequisite tasks are taken from the [Kubernetes Production Environment docs](https://kubernetes.io/docs/setup/production-environment/). The master list is really all contained with [Installing kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/).

#### Container Runtime

For all servers in the cluster:

1. Enable IPv4 packet forwarding

   Most Kubernetes cluster networking implementations need IPv4 packets to be routed between interfaces. The Linux kernel by default does not enable this, set it manually.

   ```bash
   # sysctl params required by setup, params persist across reboots
   cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
   net.ipv4.ip_forward = 1
   EOF

   # Apply sysctl params without reboot
   sudo sysctl --system
   ```

   Verify that `net.ipv4.ip_forward` is set to 1 with:

   ```bash
   sysctl net.ipv4.ip_forward
   ```

2. Install CRI runtime `containerd`

   This cluster will use the `containerd` runtime, and will install this runtime using the `apt` package manager on Ubuntu. First Docker's official GPG key is added to the `apt` sources so the latest version of `containerd` can be installed:

   a. Setup Docker's `apt` repository

   ```bash
   # Add Docker's official GPG key:
   sudo apt-get update
   sudo apt-get install ca-certificates curl
   sudo install -m 0755 -d /etc/apt/keyrings
   sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
   sudo chmod a+r /etc/apt/keyrings/docker.asc

   # Add the repository to Apt sources:
   echo \
     "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
     $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
     sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
   sudo apt-get update
   ```

   b. Install the runtime

   ```bash
   sudo apt-get install containerd.io
   ```

   c. Reset/initialize the runtime config `/etc/containerd/config.toml`

   Initialize the runtime config file

   ```bash
   # NOTE: Need to be root user, call `sudo -i` first to assume root
   containerd config default > /etc/containerd/config.toml
   ```

   Set the `SystemdCGroup = true` & `sandbox_image = "registry.k8s.io/pause:3.10"`

   ```bash
   [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
     SystemdCgroup = true # <-- Ensure this is set to true, it will be false by default!
   [plugins."io.containerd.grpc.v1.cri"]
     sandbox_image = "registry.k8s.io/pause:3.10" # <-- Ensure this is set to v3.10
   ```

   d. Restart the `containerd` service

   ```bash
   sudo systemctl restart containerd
   ```

3. Install `kubeadm`, `kubelet`, and `kubectl`

   a. Update `apt` and add the packages needed to use Kubernetes `apt` repository

   ```bash
   sudo apt-get update
   sudo apt-get install -y apt-transport-https ca-certificates curl gpg
   ```

   b. Download public signing key for Kubernetes package repositories.

   ```bash
   # If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
   # sudo mkdir -p -m 755 /etc/apt/keyrings
   curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
   ```

   c. Add the appropriate Kubernetes `apt` repository

   ```bash
   # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
   echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
   ```

   d. Update the `apt` package index, install kubelet, kubeadm, and kubectl, and pin their version:

   ```bash
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl
   ```

   e. Enable the kubelet service before running kubeadm

   ```bash
   sudo systemctl enable --now kubelet
   ```

4. Create the control plane node `kubeadm init` & copy the join joken

5. Install a CNI (using Cilium) - ONLY ON 1 CONTROL PLANE NODE

   ```bash
   CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
   CLI_ARCH=amd64
   if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
   curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
   sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
   sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
   rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
   ```

   > NOTE: For the future, this really needs to be rethought out, first we should probably use `Helm` to install `Cilium` to keep things consistent. Next there are a bunch of things that should be configured on the Cluster and as part of the install to enable features such as Gateway API we need.
   >
   > Cilium also offers Hubble for observability, this looks pretty nice - we could incorporate this into the Grafana LGTM stack as well. Is this worth it, should the configuration be make to do this too?

6. Enable Cilium hubble observability

   ```bash
   # enable hubble
   cilium hubble enable

   # wait for status to be valid
   cilium status --wait
   ```

   Install hubble CLI

   ```bash
   HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)
   HUBBLE_ARCH=amd64
   if [ "$(uname -m)" = "aarch64" ]; then HUBBLE_ARCH=arm64; fi
   curl -L --fail --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-${HUBBLE_ARCH}.tar.gz{,.sha256sum}
   sha256sum --check hubble-linux-${HUBBLE_ARCH}.tar.gz.sha256sum
   sudo tar xzvfC hubble-linux-${HUBBLE_ARCH}.tar.gz /usr/local/bin
   rm hubble-linux-${HUBBLE_ARCH}.tar.gz{,.sha256sum}
   ```

   Port forward hubble relay

   ```bash
   cilium hubble port-forward&
   ```

   NOTE: There is more to do here with observability, but not top priority at the moment. Will come back to this and flesh out full install with an Ansible playbook probably.

```bash
# install cilium as the CNI for the cluster
cilium install
```

```bash
# wait for cilium to finish setting up
cilium status --wait
```

---

# Notes

## Cloudflare Firewall Script

```bash
#!/bin/bash

# Flush only HTTP/S rules instead of all firewall rules
iptables -D INPUT -p tcp --dport 80 -j DROP 2>/dev/null
iptables -D INPUT -p tcp --dport 443 -j DROP 2>/dev/null

# Allow Cloudflare IPs (IPv4)
for ip in $(curl -s https://www.cloudflare.com/ips-v4); do
    iptables -A INPUT -p tcp -s $ip --dport 80 -j ACCEPT
    iptables -A INPUT -p tcp -s $ip --dport 443 -j ACCEPT
done

# Allow Cloudflare IPs (IPv6)
for ip in $(curl -s https://www.cloudflare.com/ips-v6); do
    ip6tables -A INPUT -p tcp -s $ip --dport 80 -j ACCEPT
    ip6tables -A INPUT -p tcp -s $ip --dport 443 -j ACCEPT
done

# Drop all other HTTP/S traffic
iptables -A INPUT -p tcp --dport 80 -j DROP
iptables -A INPUT -p tcp --dport 443 -j DROP
```

## Setup

- [ ] Create an Ansible playbook with the full setup for a control plane/worker node.
- [ ] When we join nodes to a cluster, can we do this with a service name instead of IP? Would this make the cluster more portable in my case where the physical server may need to move?
