# Ansible

To quickly recap section [01.homelab](./01.homelab.md), Ansible environments have three main components:

- **Control node** - A system on which Ansible is installed. You run Ansible commands such as `ansible` or `ansible-inventory` on a control node.
- **Inventory** - A list of managed nodes that are logically organized. You create an inventory on the control node to describe host deployments to Ansible.
- **Managed node** - A remote system, or host, that Ansible controls.

Ansible works by pushing commands from a control node to the managed nodes over an SSH connection, it does not require any software to be installed or running on the worker nodes.

![Ansible Diagram](./files/01.homelab.md/ansible_diagram.light.svg#gh-light-mode-only)
![Ansible Diagram](./files/01.homelab.md/ansible_diagram.dark.svg#gh-dark-mode-only)

> The control node does NOT need to be a linux server in your cluster, or even a linux server at all! I am using my macbook as the control node from which I run all Ansible commands. Every linux server in the cluster is a managed node on my homelab.

# Setup

To setup Ansible up, we have a few manual steps - some for the Control Node, and some for the Managed Nodes:

## Control Node

For the control node I am using my macbook, this is the computer I will use to provision all servers.

1. Install Ansible on the control node.

```bash
brew install ansible
```

2. Create an SSH keypair (or use an existing one) that will be used to authenticate your control node from the perspective of your servers.

```bash
ssh-keygen -t ed25519 -C spencerduball@gmail.com -f ~/.ssh/slduball-macbook-16
```

3. For each server, copy the full contents of the public key onto a newline in the `~/.ssh/authorized_keys` file. Every server will already have this file created. By doing this, we don't need to use a password to SSH into the servers.

```bash
# macos only command, just copying the contents of the public key to clipboard
cat ~/.ssh/slduball-macbook-16.pub | pbcopy
```

4. Create an `ansible/` directory (this git repo has one) where we will configure the ansible `inventory.yaml` and `ansible.cfg`. This is the initial config to get going, as more setup occurs - these files will evolve.

```toml
# ansible.cfg
[defaults]
inventory = inventory.yaml
roles_path = roles

[ssh_connection]
# SSH pipelining makes Ansible much faster by sending commands directly over SSH instead
# of copying a temporary file to the remote machine, then executing it via SSH, and then
# deleting it after finished.
pipelining = true
```

```yaml
# The main sections are the control nodes and the worker nodes. We also specify the type
# of node. This is helpful as each type (ex: "turing_rk1") may have a specific ISA and OS.
control_turing_rk1:
  vars:
    ansible_user: ubuntu
    ansible_python_interpreter: /usr/bin/python3
  hosts:
    node_01:
      ansible_host: 192.168.4.75

worker_turing_rk1:
  vars:
    ansible_user: ubuntu
    ansible_python_interpreter: /usr/bin/python3
  hosts:
    node_02:
      ansible_host: 192.168.4.76
    node_03:
      ansible_host: 192.168.4.78
    node_04:
      ansible_host: 192.168.4.77

# define the metagroups
control:
  children:
    control_turing_rk1:
worker:
  children:
    worker_turing_rk1:
```

5. Finally ping the servers with Ansible to ensure everything is working correctly. Make sure to execute commands from within the [`ansible/`](/ansible/) folder so that the `ansible.cfg` is discovered automatically.

> You need to add the SSH public keys before trying to ping the servers or you won't be able to connect. Perform the setup tasks for [Worker Nodes](#worker-nodes) quick and then come back here to validate.

```bash
ansible all -m ping
```

## Worker Nodes

For each worker node, all we need to do is add the SSH public key as noted in the [Control Node](#control-node) section. The rest of the server config will be automated through Ansible.

1. From the control node, copy the contents of the SSH public key file (`~/.ssh/slduball-macbook-16`)
2. SSH onto each worker node and paste this in the `~/.ssh/authorized_keys` file on a newline. This file is line separated list of the authorized SSH public keys. Here is an example below of my `~/.ssh/authorized_keys` file:

```bash
ubuntu@ubuntu:~$ cat ~/.ssh/authorized_keys
ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICabRYNtTJOdGjJhvbT4UXFGFq1QpnZ8nfuTYYjMYZUI slduball-macbook-16
```

# Usage

Ansible is an excellent tool to simplify our lives by automating the provisioning of the servers in our homelab, Ansible can also be endless in it's depth+breadth+configuration. This section will explain how Ansible is setup for my usage, and the subset of commands used to configure everything needed for this homelab. This section is not going to be an introduction to Ansible and for information about this tool specifically the two Wolfgang YouTube videos from the [References](#references) section are excellent for getting you from Ansible beginner to knowledgeable.

All Ansible commands are intended to be run from the `ansible/` folder. When running commands from this folder the `ansible.cfg` will automatically be picked up. Here is how my Ansible folder is setup:

```bash
├── ansible.cfg                # <-- The ansible configuration
├── group_vars
│   └── all
│       └── secret.yaml        # <-- Holds ansible-vault encrypted secrets for "all" nodes
├── inventory.yaml             # <-- Defines all hosts
├── playbooks
│   ├── down.yaml              # <-- The playbook to return all servers back to default
│   └── up.yaml                # <-- The playbook to configure all servers
└── roles                      # <-- Holds all named roles (ex: "system") used in playbooks
    └── system
        ├── defaults           # <-- Defines default variables used in tasks
        │   └── user.yaml
        ├── handlers
        │   └── ssh.yaml       # <-- Handlers can be triggered by tasks
        └── tasks              # <-- All executable commands of a role
            ├── essential.yaml
            ├── main.yaml
            ├── ssh.yaml
            └── user.yaml
```

This Ansible configuration is setup so that there is a single command to take servers from a default installation all the way to a complete and fully functional setup ready for Kubernetes workloads. There is also a single command to destroy everything provisioned with Ansible and return back to the default linux installation state:

- `ansible-playbook playbooks/up.yaml`
- `ansible-playbook playbooks/down.yaml`

Before running these commands there are a few things will vary from system-to-system to configure:

- Configure the `inventory.yaml`
- Setup secrets

## Inventory

An example of my `inventory.yaml` file is shown in the [Setup](#setup) section, this file will contain the configuration specific to each node and tell Ansible how to group and find these nodes.

## Secrets

To work with the `secret.yaml` file there are a few commands we will fequently use, you can see the full list of commands with `ansible-vault --help`.

```bash
ansible-vault create group_vars/all/secret.yaml # create a secret file
ansible-vault edit secret.yaml                  # edit/update the secret file (will ask for password)
ansible-playbook playbook.yaml --ask-vault-pass # run a playbook and tell ansible to ask us for the vault password
```

The list of secrets necessary to set are documented here:

```yaml
cloudflare_api_key: <cf_api_key>
```

## All Servers

- Update and upgrade packages
- Install essential packages
- Disable SSH password auth
- Enable passwordless sudo for the login user

# References

- [Ansible Home Server Pt. 1 (Wolfgang)](https://www.youtube.com/watch?v=Z7p9-m4cimg)
- [Ansible Home Server Pt. 2 (Wolfgang)](https://www.youtube.com/watch?v=SvcOwBFLVLM)
- [Ansible Docs](https://docs.ansible.com/ansible/latest/getting_started/index.html)
