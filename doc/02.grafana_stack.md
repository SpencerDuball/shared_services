# Grafana Stack

The Grafana stack is made up of a few different applications that work together to form a complete observability stack. Most deployments are setup with Kubernetes so configuring a setup for Docker Swarm will take some research & extra efforts. Most of the Grafana applications are architected such that they can be deployed in a few different ways: monolithic, simple scalable, or microservices. These different configurations go from least-to-most complex and least-to-most scalable/efficient.

The monolithic pattern is not recommended for production workloads and the simple scalable is the deployment elected for use in this stack.

## Loki

Loki is a multi-tenant log aggregation application. A typical application with Loki consists of an agent to scrape & send logs to Loki, Loki will index & store the logs, and then a frontend such as Grafana or LogCLI will be used to query and retrieve the logs for inspection and analysis.

![Grafana Loki Stack](/doc/files/02.grafana_stack.md/grafana_loki_stack.png)

Loki is composed of 8 different components, and in a microservices architecture they would all be deployed separately - however this adds a good chunk of complexity and is really only needed for extra large workloads. With the simple scalable mode this is compressed into just 3 deployment targets:

- read
- write
- backend

![Loki Simple Scalable](/doc/files/02.grafana_stack.md/loki_simple_scalable.png)

Loki has so many configuration options to be specified in the Loki `config.yaml` but there are 3 key concepts for deploying to Docker Swarm that will be focused on:

- Instance discoverability/monitoring (memberlist/hash rings/kvstore)
- Chunk & Index storage (S3)
- Stateful replicas in Docker Swarm

### Memberlists/Hash Rings/KV-Store

Loki uses the [memberlist](https://github.com/hashicorp/memberlist) package which implements gossip-based communication for cluster discovery and failure detection. Loki also uses consistent hash rings to connect individual instances when there are multiple read or write components of the same type. The hash ring uses a kvstore to provide persistent tracking of the cluster state.

In order for these instances to be connected together we can use the Docker Swarm `task.<service_name>` format which resolves to the DNS names for the individual containers of the service:

```yaml
# Memberlist for clustering: gossip-based membership protocol for finding and tracking nodes
memberlist:
  join_members: ["tasks.loki_read", "tasks.loki_write", "tasks.loki_backend"]
```

We will also specify the ring kvstore to use `memberlist` for all hash ring implementers:

```yaml
common:
  ring:
    kvstore:
      store: memberlist # Use memberlist as key-value store for gossip protocol
```

> Like hashbrowns except ring shaped.
> <br/>- Breanna

### Chunk & Index Storage (S3)

We will be using Minio, an S3 compatible object store, for all of the long term storage of chunks and indexes. In the next section we will see that unfortunately Loki is not a stateless deployment and we need to implement volumes, so S3 is not the only store we need to maintain.

### WAL and Stateful Components

Unfortunately Loki is not a stateless deployment so we need volumes for persistent storage of the WAL on the ingestor component. This is part of the "read" target and keeps a temporary in-memory store of data that it has acknowledged. In the event of a crash this data would be lost, but with the presence of a persisted WAl, this data can be recovered in the event of a crash. This is the only persistent storage that is needed for Loki. [Read more here](https://grafana.com/docs/loki/latest/operations/storage/wal/).

Since Docker Swarm does not have any viable block storage plugins, a tradeoff is made that we will tolerate the loss of data in the event of a catastrophic crash in which the node cannot be recovered at all. Using the WAL with Docker Volumes will still prevent any data loss due to updates/restarts/temporary crashes however.

Since Docker Swarm does not provide persistent volumes that can be mounted on different nodes (no viable volume plugins exist that aren't end-of-life), we will just use the "local" mount type - again stressing that the tradeoff is some data loss in the event of an unrecoverable node.

Docker Swarm also does not provide a way to have semi-persistent volumes for replicas so we will have to manually declare the volumes and ensure we purge the volumes when we scale down.
